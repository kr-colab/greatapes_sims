import numcodecs
import allel
import glob
import sys
import zarr
import numpy as np
import pandas as pd
import itertools
import argparse
from stats_helper import *
from helper_functions import *
import zarr
from Bio import SeqIO
import snakemake
import copy
import pdb

"""
import yaml
with open("stats.yaml") as f:
    config = yaml.load(f, Loader=yaml.FullLoader)
print(config)
"""

chroms = pd.read_csv(config["chr_meta_path"], sep="\t", header=None, names=["chr", "clen"])
cnames = ["chr" + str(i) for i in range(1, 23)]
chroms = chroms.loc[[c in cnames for c in chroms.chr]]

edges = pd.read_csv(config["edges_path"], sep="\t")
edges["edge"] = edges["edge"].str.replace('_', '-')
edges["parent"] = edges["parent"].str.replace('_', '-')
edges.parent= edges.parent.fillna("")
tree = build_tree_from_df(edges)
print(tree, flush=True)

vcfs = glob.glob(f'{config["vcf_path"]}*.vcf.gz')
base_names = [vcf.split("/")[-1][0:-len(".vcf.gz")] for vcf in vcfs]

if config["meta_path"]:
    meta = pd.read_csv(config["meta_path"], sep="\t")
    spp_subspp = meta[["spp", "subspp"]]
    spp_subspp = spp_subspp[spp_subspp.subspp != 'other']
    spp_subspp.drop_duplicates(inplace=True)
    spp_to_sbp = {sp: spp_subspp[spp_subspp.spp == sp].subspp.to_list() for sp in spp_subspp.spp.unique()}
else:
    spp_to_sbp = {b:b for b in base_names}

# converting to zarr
vcf_to_zarr(vcfs, base_names, config["zarr_path"], chroms.chr)

# acc masks
print("loading accessibility mask")
add_acc_group(config["zarr_path"], config["mask_path"], chroms, spp_to_sbp)

#opening callsets
callsets = { pop: zarr.open_group(f'{config["zarr_path"]}{pop}', mode='r') for pop in base_names }
merged_zarr = None
if config["merged_mask"]:
    merged_path = f'{config["zarr_path"]}merged_accessibility'
    if not os.path.exists(merged_path):
        zarr.group(merged_path)
    merged_zarr = zarr.open_group(merged_path, mode = 'r+')
    if not all([c in merged_zarr for c in chroms.chr]):
        dic_acc = {}
        for pop in callsets.keys():
            for c in chroms.chr:
                print(pop, c)
                if c not in dic_acc:
                    dic_acc[c] = callsets[pop][c]['accessibility'][:]
                else:
                    dic_acc[c] = dic_acc[c] * callsets[pop][c]['accessibility'][:]
        for c in dic_acc.keys():
            merged_zarr.create_dataset(c, shape=dic_acc[c].shape, data=dic_acc[c])

# opening ancestral alleles zarr root
anc_root = zarr.open_group(config['ancestral_zarr_path'], mode='a')

def calc_pidxy(c, clen, pop1, callsets, wsize, classes, anc_states_zarr, curr, tree, merged_zarr = None):
    pop1dash = pop1.replace('_','-')
    if pop1 == "homo":
        pop1dash = "humans"
    if pop1dash == "western-gorilla":
        pop1dash = "western-gorila"
    # if using anc_states_zarr must also pass tree (dendropy tree with taxon labels)
    df = pd.DataFrame(columns=['value', 'stat','spp1', 'spp2', 'chr', 'start', 'end', 'n_acc_bases', 'n_snps'])
    if merged_zarr is not None:
        acc1 = merged_zarr[c][:]
    else:
        acc1 = callsets[pop1][c]["accessibility"][:]
    if classes != "all":
        is_mut_class = np.full(acc1.shape, False)
        # multiple states within a class are separated by -
        for mut_class in classes.split("-"):
            is_mut_class = np.logical_or(anc_states_zarr[c][pop1dash][mut_class][:], is_mut_class)
        acc1 = np.logical_and(acc1, is_mut_class)
    print(f'We get {np.sum(acc1)} accessible bases for {pop1dash}', flush=True)
    gts1, pos1, ref1, alt1, ngts1, vcf_features = callset_to_masked_gt(callsets[pop1], c, acc1)
    ac1 = gts1.count_alleles()
    pi1, windows1, n_bases1, counts1 = allel.windowed_diversity(pos1, ac1, size=wsize, start=1, stop=clen, is_accessible=acc1)
    tmp = pd.DataFrame()
    tmp['value'] = pi1
    tmp['stat'] = 'pi'
    tmp['spp1'] = pop1
    tmp['spp2'] = pop1
    tmp['chr'] = c
    tmp['start'] = windows1[:,0]
    tmp['end'] = windows1[:,1]
    tmp['n_acc_bases'] = n_bases1
    tmp['n_snps'] = counts1
    df = pd.concat([df, tmp])
    for ft, values in vcf_features.items():
        tmp = pd.DataFrame()
        tmp['value'], _, counts = allel.windowed_statistic(pos1, values, np.nanmean, size=wsize, start=1, stop=clen)
        tmp['stat'] = ft
        tmp['chr'] = c
        tmp['start'] = windows1[:,0]
        tmp['end'] = windows1[:,1]
        tmp['spp1'] = pop1
        tmp['spp2'] = pop1
        tmp['n_snps'] = counts
        tmp['n_acc_bases'] = n_bases1
        df = pd.concat([df, tmp])
    print(f"{c}/{pop1} diversity computed.")
    # this will ensure you don't get duplicate dxys
    pops = list(callsets.keys())
    pops.sort()
    for pop2 in pops:
        pop2dash = pop2.replace('_','-')
        if pop2 == pop1:
            break
        if merged_zarr is not None:
            macc = merged_zarr[c][:]
        else:
            macc = np.logical_and(callsets[pop1][c]["accessibility"][:],callsets[pop2][c]["accessibility"][:])
        if classes != "all":
            if pop2dash == "homo":
                pop2dash = "humans"
            if pop2dash == "western-gorilla":
                pop2dash = "western-gorila"
            tree2 = copy.deepcopy(tree)
            mrca = tree2.mrca(taxon_labels=[pop1dash, pop2dash]).taxon.label
            mrcadash = mrca.replace('_','-')
            is_mut_class = np.full(acc1.shape, False)
            # multiple states within a class are separated by -
            for mut_class in classes.split("-"):
                is_mut_class = np.logical_or(anc_states_zarr[c][mrcadash][mut_class][:], is_mut_class)
            macc = np.logical_and(macc, is_mut_class)
            print(f'Pop1{pop1dash}, pop2 {pop2dash}, mrca {mrcadash}', flush=True)
        print(f'We get {np.sum(macc)} accessible bases for {pop1dash}, {pop2dash}', flush=True)
        gts2, pos2, ref2, alt2, ngts2, _ = callset_to_masked_gt(callsets[pop2], c, macc)
        ac2 = gts2.count_alleles()
        mpos, mac1, mac2, alleles1, alleles2 = merge_allel_arrays(pos1, pos2, ac1, ac2, ngts1, ngts2, ref1, ref2, alt1, alt2)
        # both orders in allele spp1 spp2 or spp2 spp1
        allelesf = alleles1+'-'+alleles2
        allelesr = alleles2+'-'+alleles1
        if curr == "all":
            is_curr = np.full(allelesf.shape, True)
        elif curr == "other":
            is_curr = ~np.logical_or.reduce([is_curr_type(ctype,allelesf, allelesr) for ctype in curr_type.keys()])
        else:
            is_curr = is_curr_type(curr,allelesf,allelesr)
        print(f'We get {mpos.shape} variants before filtering for current state', flush=True)
        mpos, mac1,mac2 = mpos[is_curr], mac1[is_curr], mac2[is_curr]
        print(f'We get {mpos.shape} variants after filtering for current state {curr}', flush=True)
        #pdb.set_trace()
        if len(mpos) > 0:
            dxy, windows, n_bases, counts = allel.windowed_divergence(mpos, mac1, mac2, size=wsize, start=1, stop=clen, is_accessible=macc)
            assert (windows==windows1).all()
        else:
            print("No avaliable bases for this dXY", flush=True)
            dxy = np.full(pi1.shape, np.nan)
            n_bases = np.full(pi1.shape, 0)
            counts = np.full(pi1.shape, 0)
            windows = windows1
        tmp = pd.DataFrame()
        tmp['value'] = dxy
        tmp['stat'] = 'dxy'
        # ensuring spp1, spp2 is sorted alphabetically
        if pop1 > pop2:
            tmp['spp1'] = pop2
            tmp['spp2'] = pop1
        else:
            tmp['spp1'] = pop1
            tmp['spp2'] = pop2
        tmp['chr'] = c
        tmp['start'] = windows[:,0]
        tmp['end'] = windows[:,1]
        tmp['n_acc_bases'] = n_bases
        tmp['n_snps'] = counts
        df = pd.concat([df, tmp])
        print(f"{c}/{pop1} x {pop2} divergence computed.")
    return df

# note the first one excludes WS/WN/NS which are all the possible weak to strong
mutation_class_dict = {
                        "A":["A", "a"],
                        "T":["T", "t"],
                        "C":["C", "c"],
                        "G":["G", "g"]
                    }
curr_type = {
                "WWSS":["A-A/T","A-T/A", "T-A/T", "T-T/A", "T-A", "A-T", "A-A", "T-T",
                "G-G/C","G-C/G", "C-G/C", "C-C/G", "C-G", "G-C", "G-G", "C-C", "A/T-A/T", "T/A-A/T", "T/A-T/A", "C/G-C/G", "C/G-G/C", "G/C-G/C"],
                "WS":["A-A/C","A-C/A", "A-A/G", "A-G/A", "C-A/C","C-C/A", "G-A/G", 
                    "G-G/A", "A-C", "A-G", "T-T/C","T-C/T", "T-T/G", "T-G/T", "C-T/C","C-C/T", "G-T/G", "G-G/T", "T-C", "T-G",
                    "A/G-A/G", "G/A-A/G", "G/A-G/A", "G/T-G/T", "T/G-T/G", "G/T-T/G", "C/A-C/A", "A/C-A/C", "C/A-A/C", "T/C-T/C", "C/T-C/T", "C/T-T/C", ]
            }
is_curr_type =  lambda ctype,c1,c2: np.logical_or(np.in1d(c1, curr_type[ctype]), np.in1d(c2, curr_type[ctype]))
possible_curr_classes = ["WWSS", "WS", "all"]
possible_anc_classes = ["all"]
ancestral_alleles_paths = [f'{config["ancestral_zarr_path"]}/{c}/great-apes/all' for  c in chroms.chr]
chr_pop_files = [f'{config["out_path"]}{config["out_path_tmp"]}pidxy_win-size_{wsize}_merged-mask_{config["merged_mask"]}_chr_{c}_pop_{pop}_state_{state}_curr_all.tsv' for c in chroms.chr for pop in callsets.keys() for wsize in config["win_sizes"] for state in possible_anc_classes]
chr_pop_files += [f'{config["out_path"]}{config["out_path_tmp"]}pidxy_win-size_{wsize}_merged-mask_{config["merged_mask"]}_chr_{c}_pop_{pop}_state_all_curr_{curr}.tsv' for c in chroms.chr for pop in callsets.keys() for wsize in config["win_sizes"] for curr in possible_curr_classes]
merged_out_files = [f'{config["out_path"]}all_pidxy_win-size_{wsize}_merged-mask_{config["merged_mask"]}_state_{state}_curr_all.tsv' for wsize in config["win_sizes"] for state in possible_anc_classes]
merged_out_files += [f'{config["out_path"]}all_pidxy_win-size_{wsize}_merged-mask_{config["merged_mask"]}_state_all_curr_{curr}.tsv' for wsize in config["win_sizes"] for curr in possible_curr_classes]
merged_stat_files = [f'{config["out_path"]}{config["out_path_figs"]}cor-pidxy-dT_win-size_{wsize}_merged-mask_{config["merged_mask"]}_state_{state}_curr_all_prop-acc_{config["prop_acc"]}.pdf' for wsize in config["win_sizes"] for state in possible_anc_classes]
merged_stat_files += [f'{config["out_path"]}{config["out_path_figs"]}cor-pidxy-dT_win-size_{wsize}_merged-mask_{config["merged_mask"]}_state_all_curr_{curr}_prop-acc_{config["prop_acc"]}.pdf' for wsize in config["win_sizes"] for curr in possible_curr_classes]

rule all:
    input: merged_out_files + merged_stat_files + ancestral_alleles_paths 

rule build_ancestral_alleles:
    output: directory(f'{config["ancestral_zarr_path"]}/{{c}}/great-apes/all')
    resources: cpus=1, runtime=300, mem_mb=72000
    run:
        c = wildcards.c
        anc_files_suffix = ".calls.txt.gz"
        anc_files_colnames = ["chromosome","position","leaf.1","leaf.2","leaf.3","leaf.4","leaf.5","leaf.6","leaf.7",
                              "leaf.8","leaf.9","leaf.10","leaf.11","inner.12","inner.13","inner.14","inner.15",
                              "inner.16","inner.17","inner.18","inner.19","inner.20","hg18","rheMac2"]
        fas_suffix = ".fa.masked"
        node_parent = pd.read_csv(config['node_parent_path'], sep="\t")
        chrom = anc_root.create_group(c)
        anc = pd.read_csv(f'{config["ancestral_path"]}/{c}{anc_files_suffix}', compression="gzip", header=None, skiprows=1, sep="\t", names=anc_files_colnames)
        # positions are 1-based in the ancestral calls file
        anc["position"] = anc["position"]-1
        # reading the fasta (the ancestral file does not have all bases, just where snps were called)
        record = SeqIO.read(f'{config["hg18_fas_path"]}/{c}{fas_suffix}', "fasta")
        seq = np.array(list(str(record.seq)))
        del record
        for index, row in node_parent.iterrows():
            # for the leaves, we get the state at inferred for the parent, but for the inner nodes
            # then we get the state inferred at that node
            # a lot of uncertainty of the state at the great-apes node, so I'm using the rheMac2 state instead of inferred
            anc_col = row["parent"] if (row["node"].startswith("leaf") or row["name"] == "great-apes") else row["node"]
            #anc_col = row["parent"] if row["node"].startswith("leaf") else row["node"]
            focal_name = row["name"]
            print(f'{c}: Filling ancestral state for {focal_name}', flush=True)
            print(f'Ancestral column for {focal_name} is {anc_col}', flush=True)
            focal = chrom.create_group(focal_name)
            for mut_class, alleles in mutation_class_dict.items():
                print(f'\t mut_class {mut_class} alleles {alleles}', flush=True)
                # building an boolean array in which indices are the positions along the genome
                # and values are whether the site at that position had ancestral state == `state` or not
                # first, let's assume that hg18 fasta carried all the right ancestral calls
                all_tmp = np.full(seq.shape, False)
                for allele in alleles:
                    tmp = (seq == allele)
                    # now, let's see where the ancestrall alleles that were called are equal to that state
                    is_focal_anc_state = (anc[anc_col].to_numpy(dtype=str) == allele)
                    tmp[anc["position"][is_focal_anc_state]] = True
                    tmp[anc["position"][np.invert(is_focal_anc_state)]] = False
                    all_tmp = np.logical_or(all_tmp, tmp)
                focal.create_dataset(name=mut_class, shape= tmp.shape, data = all_tmp)
            sum_over_classes = np.full(focal[list(mutation_class_dict.keys())[0]].shape, 0)
            for mut_class in mutation_class_dict:
                sum_over_classes += focal[mut_class][:]
            assert np.all(sum_over_classes<=1)
            print(f'\t mut_class N')
            Ns = (sum_over_classes == 0)
            focal.create_dataset(name="N", shape= Ns.shape, data = Ns)
            print(f'\t mut_class all')
            alls = np.full(Ns.shape, True)
            focal.create_dataset(name="all", shape= alls.shape, data = alls)

rule chr_pop_pidxy:
    input: f'{config["ancestral_zarr_path"]}/{{c}}/great-apes/all'
    output: f'{config["out_path"]}{config["out_path_tmp"]}pidxy_win-size_{{wsize}}_merged-mask_{config["merged_mask"]}_chr_{{c}}_pop_{{popn}}_state_{{state}}_curr_{{curr}}.tsv'
    resources: cpus=1, runtime=360, mem_mb=12000
    run:
        df = calc_pidxy(wildcards.c, chroms[chroms.chr == wildcards.c].clen.item(), wildcards.popn, callsets, int(wildcards.wsize), wildcards.state, anc_root, wildcards.curr, tree, merged_zarr)
        df.to_csv(path_or_buf=output[0], index=False, sep="\t")

rule merge:
    input: [f'{config["out_path"]}{config["out_path_tmp"]}pidxy_win-size_{{wsize}}_merged-mask_{config["merged_mask"]}_chr_{c}_pop_{popn}_state_{{state}}_curr_{{curr}}.tsv' for c in chroms.chr for popn in callsets.keys()]
    output: f'{config["out_path"]}all_pidxy_win-size_{{wsize}}_merged-mask_{config["merged_mask"]}_state_{{state}}_curr_{{curr}}.tsv'
    resources: cpus=1, runtime=120
    run:
        dfs = [pd.read_csv(f, sep="\t") for f in input]
        full = pd.concat(dfs)
        full.to_csv(path_or_buf=output[0], index=False, sep="\t")

rule ipynb_to_r:
    input: conv_script="../R/ipynb_to_r.r", in_file=config["path_Rscript"][:-1]+"ipynb"
    output: temp(config["path_Rscript"])
    shell:
        "Rscript {input.conv_script} {input.in_file}"

rule r_analysis:
    input: in_file=f'{config["out_path"]}all_pidxy_win-size_{{wsize}}_merged-mask_{config["merged_mask"]}_state_{{state}}_curr_{{curr}}.tsv', rscript=config["path_Rscript"]
    params: outpath = config["out_path"], outpath_figs = config["out_path_figs"], prop_acc = config["prop_acc"]
    output: f'{config["out_path"]}{config["out_path_figs"]}cor-pidxy-dT_win-size_{{wsize}}_merged-mask_{config["merged_mask"]}_state_{{state}}_curr_{{curr}}_prop-acc_{config["prop_acc"]}.pdf'
    resources: cpus=2, runtime=240, mem_mb=76000
    shell:
        "Rscript {input.rscript} {input.in_file} {params.outpath} {params.outpath_figs} {params.prop_acc}" 
    
rule corrs_wsize:
    input: [f'{config["out_path"]}cor-pidxy-pidxy_win-size_{wsize}_merged-mask_{config["merged_mask"]}_prop-acc_{config["prop_acc"]}.tsv' for wsize in config["win_sizes"]]
    output: f'{config["out_path"]}{config["out_path_figs"]}cor-pidxy-pidxy-by-wsize_merged-mask_{config["merged_mask"]}_prop-acc_{config["prop_acc"]}.tsv'
    resources: cpus=2, runtime=120, mem_mb=24000
    run:
        pass
